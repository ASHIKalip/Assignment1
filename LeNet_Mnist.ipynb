{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASHIKalip/DataScience/blob/main/LeNet_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5f1cusvTB8J",
        "outputId": "f012a7d0-0e84-4632-da4a-a980c534a6ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# If TensorFlow and TensorFlow Model Optimization Toolkit are not installed, use the following to install them\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow-model-optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7RrTT1-vW1zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a6be04-87df-4b8f-9282-2ba77e4ba5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 50s 25ms/step - loss: 0.2142 - accuracy: 0.9328 - val_loss: 0.0651 - val_accuracy: 0.9799\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0705 - accuracy: 0.9775 - val_loss: 0.0642 - val_accuracy: 0.9792\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0390 - val_accuracy: 0.9874\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 0.0345 - val_accuracy: 0.9887\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0319 - accuracy: 0.9900 - val_loss: 0.0337 - val_accuracy: 0.9887\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7bf56b8ddf30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Add a channel dimension to the data (MNIST is grayscale, so add a single channel)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Define a simple LeNet model\n",
        "def create_lenet_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(16, (5, 5), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(120, activation='relu'),\n",
        "        tf.keras.layers.Dense(84, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_lenet_model()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('lenet_mnist_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "1YprsWue7zjD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply pruning to the LeNet model\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.5,\n",
        "        final_sparsity=0.8,\n",
        "        begin_step=0,\n",
        "        end_step=1000  # Adjust based on training steps\n",
        "    )\n",
        "}\n",
        "\n",
        "# Prune the model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# Add the UpdatePruningStep callback\n",
        "pruning_callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir='log_dir'),  # Optional: For logging and visualization\n",
        "]\n",
        "\n",
        "# Re-compile the pruned model\n",
        "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the pruned model with the callback\n",
        "pruned_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, callbacks=pruning_callbacks)\n",
        "#pruned_model.save('pruned_lenet_mnist.h5')\n",
        "\n",
        "# Strip pruning wrappers (necessary before quantization)\n",
        "stripped_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "# Apply quantization aware training (QAT)\n",
        "qat_model = tfmot.quantization.keras.quantize_model(stripped_model)\n",
        "\n",
        "# Re-compile the quantized model\n",
        "qat_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with quantization aware training\n",
        "qat_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)\n",
        "\n",
        "# Save the quantized model\n",
        "qat_model.save('quantized_lenet_mnist.h5')\n",
        "\n",
        "# Evaluate the quantized model on the test data\n",
        "qat_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30XzuTkd8eUC",
        "outputId": "07349c4b-e2fc-4abb-ab54-faedea85988d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "   5/1875 [..............................] - ETA: 30s - loss: 0.0110 - accuracy: 0.9875    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0150s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 40s 18ms/step - loss: 0.0495 - accuracy: 0.9852 - val_loss: 0.0396 - val_accuracy: 0.9874\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0298 - accuracy: 0.9905 - val_loss: 0.0310 - val_accuracy: 0.9896\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0236 - accuracy: 0.9926 - val_loss: 0.0307 - val_accuracy: 0.9898\n",
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 46s 22ms/step - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.0409 - val_accuracy: 0.9869\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.0330 - val_accuracy: 0.9888\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.0292 - val_accuracy: 0.9918\n",
            "  1/313 [..............................] - ETA: 9s - loss: 0.0212 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 0.0292 - accuracy: 0.9918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.029169641435146332, 0.9918000102043152]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('lenet_mnist_stripped.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "beHV3seL7kLL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRTv0CEh_PKe",
        "outputId": "d83c167e-8a40-4325-f273-c99df6544f66"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0307 - accuracy: 0.9898\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.030690917745232582, 0.989799976348877]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model.save('pruned_lenet_mnist.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5q_Vevy-umb",
        "outputId": "8196982e-b138-4728-c5d5-d65d8f3d0a56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('lenet_mnist_pruned.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "Z1QmVhxa7L5X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model.save('pruned_lenet_mnist.keras')"
      ],
      "metadata": {
        "id": "Yf4PDVag-7Tz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qat_model.save('quantized_lenet_mnist.keras')"
      ],
      "metadata": {
        "id": "o9yfQaEn_C3V"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_lenet_mnist.keras')\n",
        "model.save('model_lenet_mnist.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-dm1eS_fJL",
        "outputId": "83b066fa-ac9c-4abc-9331-bd9e9dda800e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('lenet_mnist_quantized.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "0YkkzqeR555z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import heapq\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Define a Huffman node class\n",
        "class HuffmanNode:\n",
        "    def __init__(self, frequency, symbol=None, left=None, right=None):\n",
        "        self.frequency = frequency\n",
        "        self.symbol = symbol\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.frequency < other.frequency\n",
        "\n",
        "# Create Huffman tree from weight frequencies\n",
        "def create_huffman_tree(frequency):\n",
        "    heap = []\n",
        "    for symbol, freq in frequency.items():\n",
        "        heapq.heappush(heap, HuffmanNode(freq, symbol))\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        left = heapq.heappop(heap)\n",
        "        right = heapq.heappop(heap)\n",
        "        new_node = HuffmanNode(left.frequency + right.frequency, None, left, right)\n",
        "        heapq.heappush(heap, new_node)\n",
        "\n",
        "    return heapq.heappop(heap)\n",
        "\n",
        "# Generate Huffman codes by traversing the Huffman tree\n",
        "def create_huffman_codes(root, code='', code_dict=None):\n",
        "    if code_dict is None:\n",
        "        code_dict = {}\n",
        "    if root.symbol is not None:\n",
        "        code_dict[root.symbol] = code\n",
        "    else:\n",
        "        create_huffman_codes(root.left, code + '0', code_dict)\n",
        "        create_huffman_codes(root.right, code + '1', code_dict)\n",
        "    return code_dict\n",
        "\n",
        "# Function to calculate frequencies of unique weight values\n",
        "def calculate_weight_frequencies(weights):\n",
        "    unique, counts = np.unique(weights, return_counts=True)\n",
        "    frequency = dict(zip(unique, counts))\n",
        "    return frequency\n",
        "\n",
        "# Huffman coding for Keras model\n",
        "model_file = 'quantized_lenet_mnist.h5'\n",
        "\n",
        "with h5py.File(model_file, 'r') as f:\n",
        "    # Traverse the HDF5 structure to find weights\n",
        "    for group_name, group in f.items():\n",
        "        if isinstance(group, h5py.Group):\n",
        "            for dataset_name in group.keys():\n",
        "                dataset = group[dataset_name]\n",
        "                if isinstance(dataset, h5py.Dataset):\n",
        "                    try:\n",
        "                        # Access the dataset values\n",
        "                        if 'kernel:0' in dataset_name or 'bias:0' in dataset_name:\n",
        "                            weights = dataset[:]\n",
        "                            frequency = calculate_weight_frequencies(weights)\n",
        "\n",
        "                            # Create Huffman tree and generate codes\n",
        "                            huffman_tree = create_huffman_tree(frequency)\n",
        "                            huffman_codes = create_huffman_codes(huffman_tree)\n",
        "\n",
        "                            print(f\"Huffman Codes for {group_name}/{dataset_name}: {huffman_codes}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {group_name}/{dataset_name}: {e}\")\n"
      ],
      "metadata": {
        "id": "atmsFvKiZu_6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the TFLite model\n",
        "import h5py\n",
        "import heapq\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "with open('lenet_mnist_quantized.tflite', 'rb') as f:\n",
        "    tflite_data = f.read()\n",
        "\n",
        "# Calculate frequency of bytes\n",
        "byte_frequencies = collections.Counter(tflite_data)\n",
        "\n",
        "# Create Huffman tree and generate codes\n",
        "huffman_tree = create_huffman_tree(byte_frequencies)\n",
        "huffman_codes = create_huffman_codes(huffman_tree)\n",
        "\n",
        "print(\"Huffman Codes for TFLite model:\", huffman_codes)\n",
        "\n",
        "# Apply Huffman codes to compress the TFLite model\n",
        "compressed_tflite_data = []\n",
        "for byte in tflite_data:\n",
        "    compressed_tflite_data.append(huffman_codes[byte])\n",
        "\n",
        "# Convert the compressed data to binary\n",
        "compressed_tflite_data = ''.join(compressed_tflite_data)\n",
        "\n",
        "# Save the compressed TFLite model\n",
        "with open('compressed_lenet_mnist.tflite', 'wb') as f:\n",
        "    f.write(bytes(compressed_tflite_data, 'utf-8'))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3wqo5cbG9uKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b556d8a-e1a8-4d5a-e5cc-84d651982acb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Huffman Codes for TFLite model: {130: '00000', 190: '00001', 162: '0001000', 111: '00010010', 18: '000100110', 72: '000100111000', 123: '00010011100100', 175: '000100111001010', 119: '000100111001011', 233: '0001001110011', 197: '00010011101', 182: '0001001111', 220: '0001010', 146: '0001011', 93: '00011000000', 205: '00011000001', 224: '0001100001', 142: '000110001', 214: '00011001', 68: '0001101000', 239: '00011010010', 201: '000110100110', 234: '000110100111', 105: '000110101', 16: '0001101100', 63: '0001101101', 86: '0001101110', 40: '00011011110', 7: '00011011111', 34: '000111', 211: '0010000', 23: '00100010', 243: '001000110', 102: '0010001110', 229: '00100011110', 208: '00100011111', 231: '00100100', 139: '001001010', 199: '0010010110', 248: '00100101110', 15: '001001011110', 171: '001001011111', 117: '001001100', 166: '0010011010', 28: '00100110110', 94: '001001101110', 160: '0010011011110', 27: '0010011011111', 49: '00100111', 31: '00101', 189: '0011', 21: '0100', 8: '0101', 59: '01100000', 48: '0110000100', 218: '0110000101', 203: '01100001100', 184: '01100001101', 6: '0110000111', 147: '0110001000', 237: '0110001001', 242: '0110001010', 100: '0110001011', 53: '01100011', 64: '011001000', 106: '01100100100', 209: '0110010010100', 216: '0110010010101', 69: '011001001011', 133: '01100100110', 252: '01100100111', 42: '011001010', 254: '011001011', 170: '011001100', 114: '011001101', 137: '011001110', 129: '01100111100', 167: '01100111101', 24: '01100111110', 70: '01100111111', 140: '01101', 169: '0111000', 253: '01110010', 125: '011100110', 207: '01110011100', 85: '011100111010', 212: '011100111011', 56: '0111001111', 45: '01110100', 81: '011101010', 198: '011101011', 98: '01110110', 50: '011101110', 223: '01110111100', 55: '01110111101', 43: '01110111110', 82: '011101111110', 84: '011101111111', 17: '011110', 178: '01111100', 83: '011111010', 44: '011111011', 87: '0111111', 134: '100000', 191: '100001000', 174: '1000010010', 177: '1000010011', 158: '100001010', 14: '1000010110', 75: '1000010111', 79: '10000110', 247: '10000111000', 35: '1000011100100', 73: '1000011100101', 232: '100001110011', 29: '10000111010', 38: '10000111011', 108: '100001111', 2: '10001', 141: '1001000', 113: '1001001000', 150: '1001001001', 118: '1001001010', 217: '100100101100', 244: '1001001011010', 185: '1001001011011', 36: '10010010111', 95: '100100110', 54: '100100111', 228: '1001010', 103: '10010110000', 20: '10010110001', 77: '1001011001', 215: '100101101', 186: '1001011100', 138: '1001011101', 115: '100101111', 60: '10011', 195: '101000', 202: '1010010', 193: '101001100000', 157: '1010011000010', 57: '1010011000011', 32: '10100110001', 66: '10100110010', 163: '10100110011', 4: '101001101', 240: '10100111000', 3: '101001110010', 153: '101001110011', 19: '10100111010', 210: '10100111011000', 156: '10100111011001', 121: '1010011101101', 143: '101001110111', 116: '101001111', 188: '10101', 61: '10110', 30: '101110000', 221: '1011100010', 172: '101110001100', 109: '101110001101', 11: '10111000111', 52: '1011100100', 33: '1011100101', 88: '101110011', 250: '101110100', 204: '1011101010', 128: '10111010110', 173: '101110101110', 122: '101110101111', 187: '10111011', 255: '101111000', 183: '10111100100', 41: '10111100101', 159: '10111100110', 222: '10111100111', 39: '10111101', 91: '1011111000', 219: '101111100100', 196: '101111100101', 168: '101111100110', 236: '1011111001110', 89: '1011111001111', 176: '101111101', 149: '10111111', 154: '110000', 235: '11000100', 151: '11000101', 1: '110001100', 110: '110001101', 12: '11000111000', 90: '11000111001', 161: '11000111010', 112: '11000111011', 227: '110001111', 10: '1100100000', 46: '1100100001', 37: '1100100010', 144: '11001000110', 251: '11001000111', 230: '110010010000', 249: '110010010001', 92: '11001001001', 152: '110010010100', 22: '110010010101', 200: '11001001011', 127: '1100100110', 131: '1100100111', 206: '1100101', 99: '1100110', 120: '110011100', 104: '1100111010', 155: '110011101100', 25: '110011101101', 148: '11001110111', 132: '11001111000', 225: '11001111001', 126: '1100111101', 238: '1100111110', 47: '1100111111', 145: '11010000000', 80: '11010000001', 65: '110100000100', 9: '1101000001010', 13: '1101000001011', 245: '11010000011000', 135: '11010000011001', 78: '1101000001101', 76: '110100000111', 58: '110100001', 124: '1101000100', 136: '11010001010', 51: '110100010110', 180: '110100010111', 107: '1101000110', 192: '1101000111', 67: '1101001', 62: '110101', 74: '110110', 101: '110111000', 97: '110111001', 71: '1101110100', 179: '11011101010', 194: '11011101011000', 241: '11011101011001', 226: '11011101011010', 96: '11011101011011', 5: '110111010111', 26: '1101110110', 213: '110111011100', 181: '110111011101', 246: '110111011110', 165: '110111011111', 164: '1101111', 0: '111'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import heapq\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "with open(\"lenet_mnist_quantized.tflite\", \"rb\") as f:\n",
        "    tflite_data = f.read()\n",
        "\n",
        "# Step 1: Calculate frequency of each byte in the model\n",
        "byte_frequencies = collections.Counter(tflite_data)\n",
        "\n",
        "# Step 2: Create a Huffman Tree\n",
        "class HuffmanNode:\n",
        "    def __init__(self, byte, frequency):\n",
        "        self.byte = byte\n",
        "        self.frequency = frequency\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        # Compare nodes by frequency\n",
        "        return self.frequency < other.frequency\n",
        "\n",
        "def build_huffman_tree(frequencies):\n",
        "    heap = [HuffmanNode(byte, freq) for byte, freq in frequencies.items()]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        left = heapq.heappop(heap)\n",
        "        right = heapq.heappop(heap)\n",
        "\n",
        "        merged = HuffmanNode(None, left.frequency + right.frequency)\n",
        "        merged.left = left\n",
        "        merged.right = right\n",
        "\n",
        "        heapq.heappush(heap, merged)\n",
        "\n",
        "    return heap[0]\n",
        "\n",
        "# Step 3: Generate Huffman codes from the Huffman Tree\n",
        "def generate_huffman_codes(node, prefix=\"\", code_map=None):\n",
        "    if code_map is None:\n",
        "        code_map = {}\n",
        "\n",
        "    if node.byte is not None:\n",
        "        # It's a leaf node, add it to the code map\n",
        "        code_map[node.byte] = prefix\n",
        "    else:\n",
        "        # It's an internal node, recurse on children\n",
        "        generate_huffman_codes(node.left, prefix + \"0\", code_map)\n",
        "        generate_huffman_codes(node.right, prefix + \"1\", code_map)\n",
        "\n",
        "    return code_map\n",
        "\n",
        "# Build the Huffman tree and generate the codes\n",
        "huffman_tree = build_huffman_tree(byte_frequencies)\n",
        "huffman_codes = generate_huffman_codes(huffman_tree)\n",
        "\n",
        "# Step 4: Compress the TensorFlow Lite model using Huffman codes\n",
        "compressed_tflite_data = []\n",
        "for byte in tflite_data:\n",
        "    compressed_tflite_data.append(huffman_codes[byte])\n",
        "\n",
        "# Step 5: Convert the compressed data to a binary representation\n",
        "compressed_tflite_binary = \"\".join(compressed_tflite_data)\n",
        "\n",
        "# Convert the binary string into bytes\n",
        "compressed_tflite_bytes = int(compressed_tflite_binary, 2).to_bytes(\n",
        "    (len(compressed_tflite_binary) + 7) // 8, \"big\"\n",
        ")\n",
        "\n",
        "# Step 6: Save the compressed data to a file\n",
        "with open(\"compressed_lenet_mnist.tflite\", \"wb\") as f:\n",
        "    f.write(compressed_tflite_bytes)\n",
        "\n",
        "print(\"Huffman compression completed and saved to 'compressed_lenet_mnist.tflite'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THYgkQ9fqtex",
        "outputId": "c5125f04-047d-4de2-be9b-575f55f0c090"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Huffman compression completed and saved to 'compressed_lenet_mnist.tflite'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-xhIt658EcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_u1EawTX8EfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zCoRsX68Eip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fEtB2U-n8ElV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOCOpgSy8EoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYCnRmLK8Eqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK4Eo8dy8Etb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWnw1puk8Ew-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxRJs1i04FRF7iQIS4gYhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}